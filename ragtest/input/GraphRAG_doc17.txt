Configuring GraphRAG Indexing
The GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine.

Default Configuration Mode
The default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via:

Init command (recommended)
Purely using environment variables
Using JSON or YAML for deeper control
Custom Configuration Mode
Custom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the Custom Configuration Mode documentation.


Configuring GraphRAG Indexing
To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.

Usage
python -m graphrag.index [--init] [--root PATH]

Options
--init - Initialize the directory with the necessary configuration files.
--root PATH - The root directory to initialize. Default is the current directory.
Example
python -m graphrag.index --init --root ./ragtest

Output
The init command will create the following files in the specified directory:

settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation.








Default Configuration Mode (using Env Vars)
Text-Embeddings Customization
By default, the GraphRAG indexer will only emit embeddings required for our query methods. However, the model has embeddings defined for all plaintext fields, and these can be generated by setting the GRAPHRAG_EMBEDDING_TARGET environment variable to all.

If the embedding target is all, and you want to only embed a subset of these fields, you may specify which embeddings to skip using the GRAPHRAG_EMBEDDING_SKIP argument described below.

Embedded Fields
text_unit.text
document.raw_content
entity.name
entity.description
relationship.description
community.title
community.summary
community.full_content
Input Data
Our pipeline can ingest .csv or .txt data from an input folder. These files can be nested within subfolders. To configure how input data is handled, what fields are mapped over, and how timestamps are parsed, look for configuration values starting with GRAPHRAG_INPUT_ below. In general, CSV-based data provides the most customizeability. Each CSV should at least contain a text field (which can be mapped with environment variables), but it's helpful if they also have title, timestamp, and source fields. Additional fields can be included as well, which will land as extra fields on the Document table.

Base LLM Settings
These are the primary settings for configuring LLM connectivity.

Parameter	Required?	Description	Type	Default Value
GRAPHRAG_API_KEY	Yes for OpenAI. Optional for AOAI	The API key. (Note: `OPENAI_API_KEY is also used as a fallback). If not defined when using AOAI, managed identity will be used.	str	None
GRAPHRAG_API_BASE	For AOAI	The API Base URL	str	None
GRAPHRAG_API_VERSION	For AOAI	The AOAI API version.	str	None
GRAPHRAG_API_ORGANIZATION		The AOAI organization.	str	None
GRAPHRAG_API_PROXY		The AOAI proxy.	str	None
Text Generation Settings
These settings control the text generation model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.

Parameter	Required?	Description	Type	Default Value
GRAPHRAG_LLM_TYPE	For AOAI	The LLM operation type. Either openai_chat or azure_openai_chat	str	openai_chat
GRAPHRAG_LLM_DEPLOYMENT_NAME	For AOAI	The AOAI model deployment name.	str	None
GRAPHRAG_LLM_API_KEY	Yes (uses fallback)	The API key. If not defined when using AOAI, managed identity will be used.	str	None
GRAPHRAG_LLM_API_BASE	For AOAI (uses fallback)	The API Base URL	str	None
GRAPHRAG_LLM_API_VERSION	For AOAI (uses fallback)	The AOAI API version.	str	None
GRAPHRAG_LLM_API_ORGANIZATION	For AOAI (uses fallback)	The AOAI organization.	str	None
GRAPHRAG_LLM_API_PROXY		The AOAI proxy.	str	None
GRAPHRAG_LLM_MODEL		The LLM model.	str	gpt-4-turbo-preview
GRAPHRAG_LLM_MAX_TOKENS		The maximum number of tokens.	int	4000
GRAPHRAG_LLM_REQUEST_TIMEOUT		The maximum number of seconds to wait for a response from the chat client.	int	180
GRAPHRAG_LLM_MODEL_SUPPORTS_JSON		Indicates whether the given model supports JSON output mode. True to enable.	str	None
GRAPHRAG_LLM_THREAD_COUNT		The number of threads to use for LLM parallelization.	int	50
GRAPHRAG_LLM_THREAD_STAGGER		The time to wait (in seconds) between starting each thread.	float	0.3
GRAPHRAG_LLM_CONCURRENT_REQUESTS		The number of concurrent requests to allow for the embedding client.	int	25
GRAPHRAG_LLM_TOKENS_PER_MINUTE		The number of tokens per minute to allow for the LLM client. 0 = Bypass	int	0
GRAPHRAG_LLM_REQUESTS_PER_MINUTE		The number of requests per minute to allow for the LLM client. 0 = Bypass	int	0
GRAPHRAG_LLM_MAX_RETRIES		The maximum number of retries to attempt when a request fails.	int	10
GRAPHRAG_LLM_MAX_RETRY_WAIT		The maximum number of seconds to wait between retries.	int	10
GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION		Whether to sleep on rate limit recommendation. (Azure Only)	bool	True
GRAPHRAG_LLM_TEMPERATURE		The temperature to use generation.	float	0
GRAPHRAG_LLM_TOP_P		The top_p to use for sampling.	float	1
GRAPHRAG_LLM_N		The number of responses to generate.	int	1
Text Embedding Settings
These settings control the text embedding model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.

Parameter	Required ?	Description	Type	Default
GRAPHRAG_EMBEDDING_TYPE	For AOAI	The embedding client to use. Either openai_embedding or azure_openai_embedding	str	openai_embedding
GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME	For AOAI	The AOAI deployment name.	str	None
GRAPHRAG_EMBEDDING_API_KEY	Yes (uses fallback)	The API key to use for the embedding client. If not defined when using AOAI, managed identity will be used.	str	None
GRAPHRAG_EMBEDDING_API_BASE	For AOAI (uses fallback)	The API base URL.	str	None
GRAPHRAG_EMBEDDING_API_VERSION	For AOAI (uses fallback)	The AOAI API version to use for the embedding client.	str	None
GRAPHRAG_EMBEDDING_API_ORGANIZATION	For AOAI (uses fallback)	The AOAI organization to use for the embedding client.	str	None
GRAPHRAG_EMBEDDING_API_PROXY		The AOAI proxy to use for the embedding client.	str	None
GRAPHRAG_EMBEDDING_MODEL		The model to use for the embedding client.	str	text-embedding-3-small
GRAPHRAG_EMBEDDING_BATCH_SIZE		The number of texts to embed at once. (Azure limit is 16)	int	16
GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS		The maximum tokens per batch (Azure limit is 8191)	int	8191
GRAPHRAG_EMBEDDING_TARGET		The target fields to embed. Either required or all.	str	required
GRAPHRAG_EMBEDDING_SKIP		A comma-separated list of fields to skip embeddings for . (e.g. 'relationship.description')	str	None
GRAPHRAG_EMBEDDING_THREAD_COUNT		The number of threads to use for parallelization for embeddings.	int	
GRAPHRAG_EMBEDDING_THREAD_STAGGER		The time to wait (in seconds) between starting each thread for embeddings.	float	50
GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS		The number of concurrent requests to allow for the embedding client.	int	25
GRAPHRAG_EMBEDDING_TOKENS_PER_MINUTE		The number of tokens per minute to allow for the embedding client. 0 = Bypass	int	0
GRAPHRAG_EMBEDDING_REQUESTS_PER_MINUTE		The number of requests per minute to allow for the embedding client. 0 = Bypass	int	0
GRAPHRAG_EMBEDDING_MAX_RETRIES		The maximum number of retries to attempt when a request fails.	int	10
GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT		The maximum number of seconds to wait between retries.	int	10
GRAPHRAG_EMBEDDING_TARGET		The target fields to embed. Either required or all.	str	required
GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION		Whether to sleep on rate limit recommendation. (Azure Only)	bool	True
Input Settings
These settings control the data input used by the pipeline. Any settings with a fallback will use the base LLM settings, if available.

Plaintext Input Data (GRAPHRAG_INPUT_FILE_TYPE=text)
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_INPUT_FILE_PATTERN	The file pattern regexp to use when reading input files from the input directory.	str	optional	.*\.txt$
CSV Input Data (GRAPHRAG_INPUT_FILE_TYPE=csv)
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_INPUT_TYPE	The input storage type to use when reading files. (file or blob)	str	optional	file
GRAPHRAG_INPUT_FILE_PATTERN	The file pattern regexp to use when reading input files from the input directory.	str	optional	.*\.txt$
GRAPHRAG_INPUT_SOURCE_COLUMN	The 'source' column to use when reading CSV input files.	str	optional	source
GRAPHRAG_INPUT_TIMESTAMP_COLUMN	The 'timestamp' column to use when reading CSV input files.	str	optional	None
GRAPHRAG_INPUT_TIMESTAMP_FORMAT	The timestamp format to use when parsing timestamps in the timestamp column.	str	optional	None
GRAPHRAG_INPUT_TEXT_COLUMN	The 'text' column to use when reading CSV input files.	str	optional	text
GRAPHRAG_INPUT_DOCUMENT_ATTRIBUTE_COLUMNS	A list of CSV columns, comma-separated, to incorporate as document fields.	str	optional	id
GRAPHRAG_INPUT_TITLE_COLUMN	The 'title' column to use when reading CSV input files.	str	optional	title
GRAPHRAG_INPUT_STORAGE_ACCOUNT_BLOB_URL	The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net	str	optional	None
GRAPHRAG_INPUT_CONNECTION_STRING	The connection string to use when reading CSV input files from Azure Blob Storage.	str	optional	None
GRAPHRAG_INPUT_CONTAINER_NAME	The container name to use when reading CSV input files from Azure Blob Storage.	str	optional	None
GRAPHRAG_INPUT_BASE_DIR	The base directory to read input files from.	str	optional	None
Data Mapping Settings
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_INPUT_FILE_TYPE	The type of input data, csv or text	str	optional	text
GRAPHRAG_INPUT_ENCODING	The encoding to apply when reading CSV/text input files.	str	optional	utf-8
Data Chunking
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_CHUNK_SIZE	The chunk size in tokens for text-chunk analysis windows.	str	optional	1200
GRAPHRAG_CHUNK_OVERLAP	The chunk overlap in tokens for text-chunk analysis windows.	str	optional	100
GRAPHRAG_CHUNK_BY_COLUMNS	A comma-separated list of document attributes to groupby when performing TextUnit chunking.	str	optional	id
GRAPHRAG_CHUNK_ENCODING_MODEL	The encoding model to use for chunking.	str	optional	The top-level encoding model.
Prompting Overrides
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE	The path (relative to the root) of an entity extraction prompt template text file.	str	optional	None
GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS	The maximum number of redrives (gleanings) to invoke when extracting entities in a loop.	int	optional	1
GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES	A comma-separated list of entity types to extract.	str	optional	organization,person,event,geo
GRAPHRAG_ENTITY_EXTRACTION_ENCODING_MODEL	The encoding model to use for entity extraction.	str	optional	The top-level encoding model.
GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE	The path (relative to the root) of an description summarization prompt template text file.	str	optional	None
GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH	The maximum number of tokens to generate per description summarization.	int	optional	500
GRAPHRAG_CLAIM_EXTRACTION_ENABLED	Whether claim extraction is enabled for this pipeline.	bool	optional	False
GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION	The claim_description prompting argument to utilize.	string	optional	"Any claims or facts that could be relevant to threat analysis."
GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE	The claim extraction prompt to utilize.	string	optional	None
GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS	The maximum number of redrives (gleanings) to invoke when extracting claims in a loop.	int	optional	1
GRAPHRAG_CLAIM_EXTRACTION_ENCODING_MODEL	The encoding model to use for claim extraction.	str	optional	The top-level encoding model
GRAPHRAG_COMMUNITY_REPORTS_PROMPT_FILE	The community reports extraction prompt to utilize.	string	optional	None
GRAPHRAG_COMMUNITY_REPORTS_MAX_LENGTH	The maximum number of tokens to generate per community reports.	int	optional	1500
Storage
This section controls the storage mechanism used by the pipeline used for emitting output tables.

Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_STORAGE_TYPE	The type of reporter to use. Options are file, memory, or blob	str	optional	file
GRAPHRAG_STORAGE_STORAGE_ACCOUNT_BLOB_URL	The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net	str	optional	None
GRAPHRAG_STORAGE_CONNECTION_STRING	The Azure Storage connection string to use when in blob mode.	str	optional	None
GRAPHRAG_STORAGE_CONTAINER_NAME	The Azure Storage container name to use when in blob mode.	str	optional	None
GRAPHRAG_STORAGE_BASE_DIR	The base path to data outputs outputs.	str	optional	None
Cache
This section controls the cache mechanism used by the pipeline. This is used to cache LLM invocation results.

Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_CACHE_TYPE	The type of cache to use. Options are file, memory, none or blob	str	optional	file
GRAPHRAG_CACHE_STORAGE_ACCOUNT_BLOB_URL	The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net	str	optional	None
GRAPHRAG_CACHE_CONNECTION_STRING	The Azure Storage connection string to use when in blob mode.	str	optional	None
GRAPHRAG_CACHE_CONTAINER_NAME	The Azure Storage container name to use when in blob mode.	str	optional	None
GRAPHRAG_CACHE_BASE_DIR	The base path to the reporting outputs.	str	optional	None
Reporting
This section controls the reporting mechanism used by the pipeline, for common events and error messages. The default is to write reports to a file in the output directory. However, you can also choose to write reports to the console or to an Azure Blob Storage container.

Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_REPORTING_TYPE	The type of reporter to use. Options are file, console, or blob	str	optional	file
GRAPHRAG_REPORTING_STORAGE_ACCOUNT_BLOB_URL	The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net	str	optional	None
GRAPHRAG_REPORTING_CONNECTION_STRING	The Azure Storage connection string to use when in blob mode.	str	optional	None
GRAPHRAG_REPORTING_CONTAINER_NAME	The Azure Storage container name to use when in blob mode.	str	optional	None
GRAPHRAG_REPORTING_BASE_DIR	The base path to the reporting outputs.	str	optional	None
Node2Vec Parameters
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_NODE2VEC_ENABLED	Whether to enable Node2Vec	bool	optional	False
GRAPHRAG_NODE2VEC_NUM_WALKS	The Node2Vec number of walks to perform	int	optional	10
GRAPHRAG_NODE2VEC_WALK_LENGTH	The Node2Vec walk length	int	optional	40
GRAPHRAG_NODE2VEC_WINDOW_SIZE	The Node2Vec window size	int	optional	2
GRAPHRAG_NODE2VEC_ITERATIONS	The number of iterations to run node2vec	int	optional	3
GRAPHRAG_NODE2VEC_RANDOM_SEED	The random seed to use for node2vec	int	optional	597832
Data Snapshotting
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_SNAPSHOT_GRAPHML	Whether to enable GraphML snapshots.	bool	optional	False
GRAPHRAG_SNAPSHOT_RAW_ENTITIES	Whether to enable raw entity snapshots.	bool	optional	False
GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES	Whether to enable top-level node snapshots.	bool	optional	False
Miscellaneous Settings
Parameter	Description	Type	Required or Optional	Default
GRAPHRAG_ASYNC_MODE	Which async mode to use. Either asyncio or threaded.	str	optional	asyncio
GRAPHRAG_ENCODING_MODEL	The text encoding model, used in tiktoken, to encode text.	str	optional	cl100k_base
GRAPHRAG_MAX_CLUSTER_SIZE	The maximum number of entities to include in a single Leiden cluster.	int	optional	10
GRAPHRAG_SKIP_WORKFLOWS	A comma-separated list of workflow names to skip.	str	optional	None
GRAPHRAG_UMAP_ENABLED	Whether to enable UMAP layouts	bool	optional	False
Privacy
|
Consumer Health Privacy
|
Terms of Use
|
Trademarks
|
©️ 2024 Microsoft
|
GitHub
|
Solution Accelerator








Default Configuration Mode (using JSON/YAML)
The default configuration mode may be configured by using a config.json or config.yml file in the data project root. If a .env file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using ${ENV_VAR} syntax.

For example:

# .env
API_KEY=some_api_key

# config.json
{
    "llm": {
        "api_key": "${API_KEY}"
    }
}
Config Sections
input
Fields
type file|blob - The input type to use. Default=file
file_type text|csv - The type of input data to load. Either text or csv. Default is text
file_encoding str - The encoding of the input file. Default is utf-8
file_pattern str - A regex to match input files. Default is .*\.csv$ if in csv mode and .*\.txt$ if in text mode.
source_column str - (CSV Mode Only) The source column name.
timestamp_column str - (CSV Mode Only) The timestamp column name.
timestamp_format str - (CSV Mode Only) The source format.
text_column str - (CSV Mode Only) The text column name.
title_column str - (CSV Mode Only) The title column name.
document_attribute_columns list[str] - (CSV Mode Only) The additional document attributes to include.
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to read input from, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
llm
This is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration.

Fields
api_key str - The OpenAI API key to use.
type openai_chat|azure_openai_chat|openai_embedding|azure_openai_embedding - The type of LLM to use.
model str - The model name.
max_tokens int - The maximum number of output tokens.
request_timeout float - The per-request timeout.
api_base str - The API base url to use.
api_version str - The API version
organization str - The client organization.
proxy str - The proxy URL to use.
cognitive_services_endpoint str - The url endpoint for cognitive services.
deployment_name str - The deployment name to use (Azure).
model_supports_json bool - Whether the model supports JSON-mode output.
tokens_per_minute int - Set a leaky-bucket throttle on tokens-per-minute.
requests_per_minute int - Set a leaky-bucket throttle on requests-per-minute.
max_retries int - The maximum number of retries to use.
max_retry_wait float - The maximum backoff time.
sleep_on_rate_limit_recommendation bool - Whether to adhere to sleep recommendations (Azure).
concurrent_requests int The number of open requests to allow at once.
temperature float - The temperature to use.
top_p float - The top-p value to use.
n int - The number of completions to generate.
parallelization
Fields
stagger float - The threading stagger value.
num_threads int - The maximum number of work threads.
async_mode
asyncio|threaded The async mode to use. Either asyncio or `threaded.

embeddings
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
batch_size int - The maximum batch size to use.
batch_max_tokens int - The maximum batch #-tokens.
target required|all - Determines which set of embeddings to emit.
skip list[str] - Which embeddings to skip.
strategy dict - Fully override the text-embedding strategy.
chunks
Fields
size int - The max chunk size in tokens.
overlap int - The chunk overlap in tokens.
group_by_columns list[str] - group documents by fields before chunking.
encoding_model str - The text encoding model to use. Default is to use the top-level encoding model.
strategy dict - Fully override the chunking strategy.
cache
Fields
type file|memory|none|blob - The cache type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write cache to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
storage
Fields
type file|memory|blob - The storage type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
reporting
Fields
type file|console|blob - The reporting type to use. Default=file
connection_string str - (blob only) The Azure Storage connection string.
container_name str - (blob only) The Azure Storage container name.
base_dir str - The base directory to write reports to, relative to the root.
storage_account_blob_url str - The storage account blob URL to use.
entity_extraction
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
entity_types list[str] - The entity types to identify.
max_gleanings int - The maximum number of gleaning cycles to use.
encoding_model str - The text encoding model to use. By default, this will use the top-level encoding model.
strategy dict - Fully override the entity extraction strategy.
summarize_descriptions
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per summarization.
strategy dict - Fully override the summarize description strategy.
claim_extraction
Fields
enabled bool - Whether to enable claim extraction. default=False
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
description str - Describes the types of claims we want to extract.
max_gleanings int - The maximum number of gleaning cycles to use.
encoding_model str - The text encoding model to use. By default, this will use the top-level encoding model.
strategy dict - Fully override the claim extraction strategy.
community_reports
Fields
llm (see LLM top-level config)
parallelization (see Parallelization top-level config)
async_mode (see Async Mode top-level config)
prompt str - The prompt file to use.
max_length int - The maximum number of output tokens per report.
max_input_length int - The maximum number of input tokens to use when generating reports.
strategy dict - Fully override the community reports strategy.
cluster_graph
Fields
max_cluster_size int - The maximum cluster size to emit.
strategy dict - Fully override the cluster_graph strategy.
embed_graph
Fields
enabled bool - Whether to enable graph embeddings.
num_walks int - The node2vec number of walks.
walk_length int - The node2vec walk length.
window_size int - The node2vec window size.
iterations int - The node2vec number of iterations.
random_seed int - The node2vec random seed.
strategy dict - Fully override the embed graph strategy.
umap
Fields
enabled bool - Whether to enable UMAP layouts.
snapshots
Fields
graphml bool - Emit graphml snapshots.
raw_entities bool - Emit raw entity snapshots.
top_level_nodes bool - Emit top-level-node snapshots.
encoding_model
str - The text encoding model to use. Default is cl100k_base.

skip_workflows
list[str] - Which workflow names to skip.
